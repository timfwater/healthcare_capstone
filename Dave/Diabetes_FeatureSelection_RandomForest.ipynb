{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiabetesAllDummy = pd.read_csv('DiabetesAllDummy.csv', index_col=0)\n",
    "#DiabetesOrdMed = pd.read_csv('DiabetesOrdMed.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a random index of 20% of the values in our dataset. This is the test dataset and will *only* be used for testing our models (sparingly!)\n",
    "np.random.seed(100)\n",
    "length = len(DiabetesAllDummy)\n",
    "testIdx = np.random.choice(range(length), size=int(round(0.2*length)), replace=False)\n",
    "trainIdx = list(set(range(length))-set(testIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just start by looking at a simple linear regression:\n",
    "#Yes, we have a categorical variable as our output. This will not be valid, but will show right away variables which could be important:\n",
    "\n",
    "DiabetesTrain = DiabetesAllDummy[DiabetesAllDummy['IsTrain']==1]\n",
    "DiabetesTrain.index = list(range(len(DiabetesTrain)))\n",
    "\n",
    "DiabetesTest = DiabetesAllDummy[DiabetesAllDummy['IsTrain']==0]\n",
    "DiabetesTest.index = list(range(len(DiabetesTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try simple K-fold (5) random forest classifier using default conditions:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc = rfc()\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrain.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrain['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTest.drop('readmitted', axis=1)\n",
    "testY = DiabetesTest['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "XXX = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    rfc.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    pxxx = rfc.predict_proba(trainX.iloc[train,])\n",
    "    p = rfc.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = rfc.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = rfc.score(testX, testY)\n",
    "    TrainScore = rfc.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    XXX.append(pxxx)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = rfc.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9803486974573493,\n",
       " 0.9801926108605054,\n",
       " 0.9794277865359702,\n",
       " 0.9797555683893424,\n",
       " 0.9798027096210277]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the internal train scores, based on the logistic regression\n",
    "TrainScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.883373915215084,\n",
       " 0.8875569707186115,\n",
       " 0.8841231191858651,\n",
       " 0.8844352875070238,\n",
       " 0.885551948051948]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the K-test scores, based on the logistic regression\n",
    "ScoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8849725411882177,\n",
       " 0.8853220169745382,\n",
       " 0.8845232151772342,\n",
       " 0.8845731402895657,\n",
       " 0.8846729905142287]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the scores for each of the 5 ensemble models, compared to the untouched 20% test index\n",
    "PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64),\n",
       " array([0, 0, 0, ..., 0, 0, 0], dtype=int64)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the prediction lists from the train, compared to actual Y values for the train:\n",
    "PredictList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8865441286649018"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These values may sound impressive, but they absolutely aren't. 11.1 percent of patients return under 30 days, so simply guessing \"No\" will get you 88.8% accuracy:\n",
    "1-np.sum(trainY)/len(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy with reduced features was no better than the overall accuracy with all features. Let's check what might be going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(PredictList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9086"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16017"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(PredictList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This seems to be the biggest problem. Out of ~16,000 observations (in the first K group, as an example), only 78 of them\\nwere predicted to return within 30 days. We know that 11% should, or rougly 1,800. We need to tune this model to be more\\ngenerous in predicting a positive outcome'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This seems to be the biggest problem. Out of ~16,000 observations (in the first K group, as an example), only 78 of them\n",
    "were predicted to return within 30 days. We know that 11% should, or rougly 1,800. We need to tune this model to be more\n",
    "generous in predicting a positive outcome'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import RandomizedSearchCV\\nfrom sklearn.ensemble import RandomForestClassifier as rfc\\n\\n#We will perform a grid search to find the optimal hyperparameters for our RF algorithm and test using our K-fold data:\\n\\n# C value is the one most important for tuning a logistic regression. Let's see how varying this value affects the score:\\nmin_samples_split = [2, 3, 5, 10]\\nmin_samples_leaf = [1, 2, 4]\\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\\nmax_depth.append(None)\\nmax_features = ['sqrt', 'log2']\\n\\n\\n# Create the random grid\\nrandom_grid = {'min_samples_split': min_samples_split, 'n_estimators':n_estimators,\\n              'max_depth':max_depth, 'max_features':max_features, 'min_samples_leaf':min_samples_leaf}\\n\\n# Use the random grid to search for best C hyperparameter:\\n\\n# First create the base model to tune\\nrftune = rfc()\\n\\n# Random search of parameters, using 5-fold cross validation, \\nrf_random = RandomizedSearchCV(estimator = rftune, scoring='f1', param_distributions = random_grid, n_iter = 100, cv = 5, \\n                               verbose=2, random_state=42, n_jobs = 3)\\n\\n# Fit the random search model\\nrf_random.fit(trainX, trainY)\\n\\n#Then print the best parameters using best_params_\\nrf_random.best_params_\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is commented out because it took a VERY long time\n",
    "'''from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "#We will perform a grid search to find the optimal hyperparameters for our RF algorithm and test using our K-fold data:\n",
    "\n",
    "# C value is the one most important for tuning a logistic regression. Let's see how varying this value affects the score:\n",
    "min_samples_split = [2, 3, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "max_features = ['sqrt', 'log2']\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'min_samples_split': min_samples_split, 'n_estimators':n_estimators,\n",
    "              'max_depth':max_depth, 'max_features':max_features, 'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "# Use the random grid to search for best C hyperparameter:\n",
    "\n",
    "# First create the base model to tune\n",
    "rftune = rfc()\n",
    "\n",
    "# Random search of parameters, using 5-fold cross validation, \n",
    "rf_random = RandomizedSearchCV(estimator = rftune, scoring='f1', param_distributions = random_grid, n_iter = 100, cv = 5, \n",
    "                               verbose=2, random_state=42, n_jobs = 3)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(trainX, trainY)\n",
    "\n",
    "#Then print the best parameters using best_params_\n",
    "rf_random.best_params_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc = rfc()\n",
    "rfc.set_params(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=60)\n",
    "\n",
    "rfc.fit(trainX, trainY)\n",
    "predictprobs = rfc.predict_proba(trainX)\n",
    "predictvalues = rfc.predict(trainX)\n",
    "actual = trainY.values\n",
    "\n",
    "R2 = rfc.score(trainX, trainY)\n",
    "#Run a test on the completely untouched test 20%\n",
    "TestR2 = rfc.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicttest = rfc.predict(testX)\n",
    "predicttestprobs = rfc.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6566596221552277"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "AUC(testY, predicttestprobs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9329079466560112\n"
     ]
    }
   ],
   "source": [
    "print(rfc.score(trainX, trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8870693959061408\n"
     ]
    }
   ],
   "source": [
    "print(rfc.score(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "21\n",
      "20030\n"
     ]
    }
   ],
   "source": [
    "print(len(predicttestprobs[:,1][predicttestprobs[:,1]>0.20]))\n",
    "#This predictor estimates that there is an 11.6 percent chance in the train set\n",
    "#np.sum(predictvalues)\n",
    "print(np.sum(predicttest))\n",
    "print(len(predicttest))\n",
    "#The test classifier predicts 20/20030 hits (less than 1%), despite >11% being positive\n",
    "#We would need to set the threshhold to about 20% to get 11% hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257\n",
      "17773\n",
      "559\n",
      "1712\n",
      "3410\n"
     ]
    }
   ],
   "source": [
    "#I'm curious how effective that cutoff (20%) would be:\n",
    "predictedY = testY[predicttestprobs[:,1]>0.20]\n",
    "predictedN = testY[predicttestprobs[:,1]<=0.2]\n",
    "\n",
    "print(len(predictedY))\n",
    "print(len(predictedN))\n",
    "print(np.sum(predictedY))\n",
    "print(np.sum(predictedN))\n",
    "\n",
    "print(np.sum((1-predictedY)**2) + np.sum(predictedN**2))\n",
    "\n",
    "#At this cutoff, we made 20030 predictions. 2281 were yes (577 right, 1704 wrong); 17749 were no (16055 right, 1694 wrong). \n",
    "#This is only 83.0 percent accurate, worse than simply guessing no for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "20009\n",
      "15\n",
      "2256\n",
      "2262\n"
     ]
    }
   ],
   "source": [
    "#I'm curious how effective that cutoff (20%) would be:\n",
    "predictedY = testY[predicttestprobs[:,1]>0.50]\n",
    "predictedN = testY[predicttestprobs[:,1]<=0.5]\n",
    "\n",
    "print(len(predictedY))\n",
    "print(len(predictedN))\n",
    "print(np.sum(predictedY))\n",
    "print(np.sum(predictedN))\n",
    "\n",
    "print(np.sum((1-predictedY)**2) + np.sum(predictedN**2))\n",
    "\n",
    "#Here, we predict 20 positives (yet still only 12 of them are actually positive... what??) and a slew of negatives, which have a standard 11% error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0964159970920667"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((predicttestprobs[:,1]-testY)**2)/len(testY)\n",
    "#This prediction is an actual 9.6% error rate, better than simply predicting \"No\" for everything. How do we capture this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Let\\'s try the RF regression with c=0.001 and see how that works:\\n\\nfrom sklearn.ensemble import RandomForestClassifier as rfc\\n\\nrfc()\\nrfc.set_params(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_features=\\'sqrt\\', max_depth=60)\\n\\nimport sklearn.model_selection as ms\\nms_k5 = ms.KFold(n_splits=5, shuffle=True)\\n\\n#First try with all features:\\n\\n#Create X and Y variables (replace 2 to 1 and 1 to 0)\\ntrainX = DiabetesTrain.drop(\\'readmitted\\', axis=1)\\ntrainY = DiabetesTrain[\\'readmitted\\'].replace([2, 1], [1, 0])\\n\\ntestX = DiabetesTest.drop(\\'readmitted\\', axis=1)\\ntestY = DiabetesTest[\\'readmitted\\'].replace([2, 1], [1, 0])\\n\\n\\n#Instantiate the K-fold generator object:\\nnp.random.seed(0)\\nDiabetesAD5Fold = ms_k5.split(trainX, trainY)\\n\\n#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\\nScoreList = []\\nPredictList = []\\nPureTestScore = []\\nTrainScores = []\\nActualValues = []\\n\\nfor train, test in DiabetesAD5Fold:\\n\\n    \\n    #Run the fit using the train data for each K\\n    lgr.fit(trainX.iloc[train,], trainY[train])\\n    #Run your predicion for the \"missing\" K-part\\n    p = lgr.predict(trainX.iloc[test,])\\n    actual = trainY[train].values\\n    #Check your schore for the missing K-part\\n    R2 = lgr.score(trainX.iloc[test,], trainY[test])\\n    #Run a test on the completely untouched test 20%\\n    TestR2 = lgr.score(testX, testY)\\n    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\\n    \\n    #Append these scores to the lists above\\n    ScoreList.append(R2)\\n    PureTestScore.append(TestR2)\\n    PredictList.append(p)\\n    TrainScores.append(TrainScore)\\n    ActualValues.append(actual)\\n    \\n    #Make predictions for the completely untouched 20%\\n    PredictionsTest = lgr.predict(testX)\\n    \\n    #Use these predictions to calculate RMSLE for the untouched 20% and append\\n    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\\n    #RMSLE.append(RMSLEvalue)'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''#Let's try the RF regression with c=0.001 and see how that works:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc()\n",
    "rfc.set_params(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=60)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrain.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrain['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTest.drop('readmitted', axis=1)\n",
    "testY = DiabetesTest['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    lgr.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = lgr.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = lgr.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = lgr.score(testX, testY)\n",
    "    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = lgr.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Either logistic regression does not work for this data, or something is going wrong here. The predicted scores are up by the slightest amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Finally, try the logistic regression with c=0.001 and reduced features and see how that works:\\n\\nfrom sklearn.linear_model import LogisticRegression as lgr\\n\\nlgr = lgr()\\nlgr.set_params(C=0.001)\\n\\nimport sklearn.model_selection as ms\\nms_k5 = ms.KFold(n_splits=5, shuffle=True)\\n\\n#First try with all features:\\n\\n\\n\\n#Create X and Y variables (replace 2 to 1 and 1 to 0)\\ntrainX = DiabetesTrainM.drop(\\'readmitted\\', axis=1)\\ntrainY = DiabetesTrainM[\\'readmitted\\'].replace([2, 1], [1, 0])\\n\\ntestX = DiabetesTestM.drop(\\'readmitted\\', axis=1)\\ntestY = DiabetesTestM[\\'readmitted\\'].replace([2, 1], [1, 0])\\n\\n\\n#Instantiate the K-fold generator object:\\nnp.random.seed(0)\\nDiabetesAD5Fold = ms_k5.split(trainX, trainY)\\n\\n#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\\nScoreList = []\\nPredictList = []\\nPureTestScore = []\\nTrainScores = []\\nActualValues = []\\n\\nfor train, test in DiabetesAD5Fold:\\n\\n    \\n    #Run the fit using the train data for each K\\n    lgr.fit(trainX.iloc[train,], trainY[train])\\n    #Run your predicion for the \"missing\" K-part\\n    p = lgr.predict(trainX.iloc[test,])\\n    actual = trainY[train].values\\n    #Check your schore for the missing K-part\\n    R2 = lgr.score(trainX.iloc[test,], trainY[test])\\n    #Run a test on the completely untouched test 20%\\n    TestR2 = lgr.score(testX, testY)\\n    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\\n    \\n    #Append these scores to the lists above\\n    ScoreList.append(R2)\\n    PureTestScore.append(TestR2)\\n    PredictList.append(p)\\n    TrainScores.append(TrainScore)\\n    ActualValues.append(actual)\\n    \\n    #Make predictions for the completely untouched 20%\\n    PredictionsTest = lgr.predict(testX)\\n    \\n    #Use these predictions to calculate RMSLE for the untouched 20% and append\\n    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\\n    #RMSLE.append(RMSLEvalue)'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Finally, try the logistic regression with c=0.001 and reduced features and see how that works:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as lgr\n",
    "\n",
    "lgr = lgr()\n",
    "lgr.set_params(C=0.001)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrainM.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrainM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTestM.drop('readmitted', axis=1)\n",
    "testY = DiabetesTestM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    lgr.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = lgr.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = lgr.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = lgr.score(testX, testY)\n",
    "    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = lgr.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.linear_model import LogisticRegression as lgr\\n\\nlgr = lgr()\\nlgr.set_params(C=1000)\\n\\nimport sklearn.model_selection as ms\\nms_k5 = ms.KFold(n_splits=5, shuffle=True)\\n\\n#First try with all features:\\n\\n\\n\\n#Create X and Y variables (replace 2 to 1 and 1 to 0)\\ntrainX = DiabetesTrainM.drop(\\'readmitted\\', axis=1)\\ntrainY = DiabetesTrainM[\\'readmitted\\'].replace([2, 1], [1, 0])\\n\\ntestX = DiabetesTestM.drop(\\'readmitted\\', axis=1)\\ntestY = DiabetesTestM[\\'readmitted\\'].replace([2, 1], [1, 0])\\n\\n\\n#Instantiate the K-fold generator object:\\nnp.random.seed(0)\\nDiabetesAD5Fold = ms_k5.split(trainX, trainY)\\n\\n#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\\nScoreList = []\\nPredictList = []\\nPureTestScore = []\\nTrainScores = []\\nActualValues = []\\n\\nfor train, test in DiabetesAD5Fold:\\n\\n    \\n    #Run the fit using the train data for each K\\n    lgr.fit(trainX.iloc[train,], trainY[train])\\n    #Run your predicion for the \"missing\" K-part\\n    p = lgr.predict(trainX.iloc[test,])\\n    actual = trainY[train].values\\n    #Check your schore for the missing K-part\\n    R2 = lgr.score(trainX.iloc[test,], trainY[test])\\n    #Run a test on the completely untouched test 20%\\n    TestR2 = lgr.score(testX, testY)\\n    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\\n    \\n    #Append these scores to the lists above\\n    ScoreList.append(R2)\\n    PureTestScore.append(TestR2)\\n    PredictList.append(p)\\n    TrainScores.append(TrainScore)\\n    ActualValues.append(actual)\\n    \\n    #Make predictions for the completely untouched 20%\\n    PredictionsTest = lgr.predict(testX)\\n    \\n    #Use these predictions to calculate RMSLE for the untouched 20% and append\\n    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\\n    #RMSLE.append(RMSLEvalue)'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, try the logistic regression with c=0.001 and reduced features and see how that works:\n",
    "\n",
    "'''from sklearn.linear_model import LogisticRegression as lgr\n",
    "\n",
    "lgr = lgr()\n",
    "lgr.set_params(C=1000)\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "ms_k5 = ms.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "#First try with all features:\n",
    "\n",
    "\n",
    "\n",
    "#Create X and Y variables (replace 2 to 1 and 1 to 0)\n",
    "trainX = DiabetesTrainM.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrainM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTestM.drop('readmitted', axis=1)\n",
    "testY = DiabetesTestM['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "\n",
    "#Instantiate the K-fold generator object:\n",
    "np.random.seed(0)\n",
    "DiabetesAD5Fold = ms_k5.split(trainX, trainY)\n",
    "\n",
    "#Produce list of test R^2 scores, and Actual vs Predicted lists for the individual runs\n",
    "ScoreList = []\n",
    "PredictList = []\n",
    "PureTestScore = []\n",
    "TrainScores = []\n",
    "ActualValues = []\n",
    "\n",
    "for train, test in DiabetesAD5Fold:\n",
    "\n",
    "    \n",
    "    #Run the fit using the train data for each K\n",
    "    lgr.fit(trainX.iloc[train,], trainY[train])\n",
    "    #Run your predicion for the \"missing\" K-part\n",
    "    p = lgr.predict(trainX.iloc[test,])\n",
    "    actual = trainY[train].values\n",
    "    #Check your schore for the missing K-part\n",
    "    R2 = lgr.score(trainX.iloc[test,], trainY[test])\n",
    "    #Run a test on the completely untouched test 20%\n",
    "    TestR2 = lgr.score(testX, testY)\n",
    "    TrainScore = lgr.score(trainX.iloc[train,], trainY[train])\n",
    "    \n",
    "    #Append these scores to the lists above\n",
    "    ScoreList.append(R2)\n",
    "    PureTestScore.append(TestR2)\n",
    "    PredictList.append(p)\n",
    "    TrainScores.append(TrainScore)\n",
    "    ActualValues.append(actual)\n",
    "    \n",
    "    #Make predictions for the completely untouched 20%\n",
    "    PredictionsTest = lgr.predict(testX)\n",
    "    \n",
    "    #Use these predictions to calculate RMSLE for the untouched 20% and append\n",
    "    #RMSLEvalue = np.sqrt(np.mean(np.power(np.log1p(testY)-np.log1p(PredictionsTest), 2)))\n",
    "    #RMSLE.append(RMSLEvalue)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PureTestScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.sum(PredictList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's just start by looking at a simple linear regression:\n",
    "#Yes, we have a categorical variable as our output. This will not be valid, but will show right away variables which could be important:\n",
    "\n",
    "\n",
    "DiabetesAllDummy = pd.read_csv('DiabetesAllDummy.csv', index_col=0)\n",
    "\n",
    "DiabetesTrain = DiabetesAllDummy[DiabetesAllDummy['IsTrain']==1].drop('IsTrain', axis=1)\n",
    "DiabetesTrain.index = list(range(len(DiabetesTrain)))\n",
    "\n",
    "DiabetesTest = DiabetesAllDummy[DiabetesAllDummy['IsTrain']==0].drop('IsTrain', axis=1)\n",
    "DiabetesTest.index = list(range(len(DiabetesTest)))\n",
    "\n",
    "trainX = DiabetesTrain.drop('readmitted', axis=1)\n",
    "trainY = DiabetesTrain['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX = DiabetesTest.drop('readmitted', axis=1)\n",
    "testY = DiabetesTest['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "#Import and run the model:\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc = rfc()\n",
    "rfc.set_params(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=60)\n",
    "\n",
    "rfc.fit(trainX, trainY)\n",
    "predictprobs = rfc.predict_proba(trainX)\n",
    "predictvalues = rfc.predict(trainX)\n",
    "actual = trainY.values\n",
    "\n",
    "R2 = rfc.score(trainX, trainY)\n",
    "#Run a test on the completely untouched test 20%\n",
    "TestR2 = rfc.score(testX, testY)\n",
    "\n",
    "predicttest = rfc.predict(testX)\n",
    "predicttestprobs = rfc.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6565731991337912"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "AUC(testY, predicttestprobs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9323460366615054\n",
      "0.8866699950074888\n"
     ]
    }
   ],
   "source": [
    "print(R2)\n",
    "print(TestR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "20013\n",
      "9\n",
      "2262\n",
      "SquaredScore: 2270\n"
     ]
    }
   ],
   "source": [
    "#I'm curious how effective that cutoff (20%) would be:\n",
    "predictedY = testY[predicttestprobs[:,1]>0.50]\n",
    "predictedN = testY[predicttestprobs[:,1]<=0.5]\n",
    "\n",
    "print(len(predictedY))\n",
    "print(len(predictedN))\n",
    "print(np.sum(predictedY))\n",
    "print(np.sum(predictedN))\n",
    "\n",
    "print('SquaredScore: ' + str(np.sum((1-predictedY)**2) + np.sum(predictedN**2)))\n",
    "\n",
    "#Here, we predict 17 positives (yet still only 10 of them are actually positive... what??) and a slew of negatives, which have a standard 11% error rate.\n",
    "#SquaredScore would have been 2271, guessing everything as \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2231\n",
      "17799\n",
      "573\n",
      "1698\n",
      "SquaredScore: 3356\n"
     ]
    }
   ],
   "source": [
    "#I'm curious how effective that cutoff (20%) would be:\n",
    "predictedY = testY[predicttestprobs[:,1]>0.20]\n",
    "predictedN = testY[predicttestprobs[:,1]<=0.2]\n",
    "\n",
    "print(len(predictedY))\n",
    "print(len(predictedN))\n",
    "print(np.sum(predictedY))\n",
    "print(np.sum(predictedN))\n",
    "\n",
    "print('SquaredScore: ' + str(np.sum((1-predictedY)**2) + np.sum(predictedN**2)))\n",
    "\n",
    "#At this cutoff, we made 20030 predictions. 2227 were yes (576 right, 1651 wrong); 17803 were no (16055 right, 1694 wrong). \n",
    "#This is only 83.0 percent accurate, worse than simply guessing no for everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11345587133509814\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(trainY)/len(trainY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquaredScore: 2013.5142942422617\n"
     ]
    }
   ],
   "source": [
    "#Is your actual prediction more accurate??  What would the squared score be just guessing the trained P (0.1134)?\n",
    "print('SquaredScore: ' + str(np.sum((0.11345587-testY)**2)))\n",
    "#You could get down to a score of 2013.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquaredScore: 1928.0117989782798\n"
     ]
    }
   ],
   "source": [
    "#Now what would the squared score be for our actual prediction?\n",
    "print('SquaredScore: ' + str(np.sum((predicttestprobs[:,1]-testY)**2)))\n",
    "#The predicted probabilities are significantly better than guessing the train probability for every value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do this same analysis for dataset2 (meds reduced from up down steady no to change steady no)\n",
    "#Set2\n",
    "\n",
    "\n",
    "DiabetesAnyChange = pd.read_csv('DiabetesAnyChange.csv', index_col=0)\n",
    "\n",
    "DiabetesTrain2 = DiabetesAnyChange[DiabetesAnyChange['IsTrain']==1].drop('IsTrain', axis=1)\n",
    "DiabetesTrain2.index = list(range(len(DiabetesTrain2)))\n",
    "\n",
    "DiabetesTest2 = DiabetesAnyChange[DiabetesAnyChange['IsTrain']==0].drop('IsTrain', axis=1)\n",
    "DiabetesTest2.index = list(range(len(DiabetesTest2)))\n",
    "\n",
    "trainX2 = DiabetesTrain2.drop('readmitted', axis=1)\n",
    "trainY2 = DiabetesTrain2['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX2 = DiabetesTest2.drop('readmitted', axis=1)\n",
    "testY2 = DiabetesTest2['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "#Import and run the model:\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc = rfc()\n",
    "rfc.set_params(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=60)\n",
    "\n",
    "rfc.fit(trainX2, trainY2)\n",
    "predictprobs = rfc.predict_proba(trainX2)\n",
    "predictvalues = rfc.predict(trainX2)\n",
    "actual = trainY2.values\n",
    "\n",
    "R2 = rfc.score(trainX2, trainY2)\n",
    "#Run a test on the completely untouched test 20%\n",
    "TestR2 = rfc.score(testX2, testY2)\n",
    "\n",
    "predicttest = rfc.predict(testX2)\n",
    "predicttestprobs = rfc.predict_proba(testX2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6585450970103685"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "AUC(testY2, predicttestprobs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquaredScore: 1933.9284406748222\n"
     ]
    }
   ],
   "source": [
    "#Now what would the squared score be for our actual prediction?\n",
    "print('SquaredScore: ' + str(np.sum((predicttestprobs[:,1]-testY)**2)))\n",
    "#The predicted probabilities are significantly better than guessing the train probability for every value.\n",
    "#Virtually the same (slightest bit better) than the AllDummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do this same analysis for dataset2 (meds reduced from up down steady no to yes no)\n",
    "#Set2\n",
    "\n",
    "\n",
    "DiabetesTakingMed = pd.read_csv('DiabetesTakingMed.csv', index_col=0)\n",
    "\n",
    "DiabetesTrain3 = DiabetesTakingMed[DiabetesTakingMed['IsTrain']==1].drop('IsTrain', axis=1)\n",
    "DiabetesTrain3.index = list(range(len(DiabetesTrain3)))\n",
    "\n",
    "DiabetesTest3 = DiabetesTakingMed[DiabetesTakingMed['IsTrain']==0].drop('IsTrain', axis=1)\n",
    "DiabetesTest3.index = list(range(len(DiabetesTest3)))\n",
    "\n",
    "trainX3 = DiabetesTrain3.drop('readmitted', axis=1)\n",
    "trainY3 = DiabetesTrain3['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "testX3 = DiabetesTest3.drop('readmitted', axis=1)\n",
    "testY3 = DiabetesTest3['readmitted'].replace([2, 1], [1, 0])\n",
    "\n",
    "#Import and run the model:\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "\n",
    "rfc = rfc()\n",
    "rfc.set_params(n_estimators=200, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=60)\n",
    "\n",
    "rfc.fit(trainX3, trainY3)\n",
    "predictprobs = rfc.predict_proba(trainX3)\n",
    "predictvalues = rfc.predict(trainX3)\n",
    "actual = trainY3.values\n",
    "\n",
    "R2 = rfc.score(trainX3, trainY3)\n",
    "#Run a test on the completely untouched test 20%\n",
    "TestR2 = rfc.score(testX3, testY3)\n",
    "\n",
    "predicttest = rfc.predict(testX3)\n",
    "predicttestprobs = rfc.predict_proba(testX3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquaredScore: 1930.9910475381848\n"
     ]
    }
   ],
   "source": [
    "#Now what would the squared score be for our actual prediction?\n",
    "print('SquaredScore: ' + str(np.sum((predicttestprobs[:,1]-testY)**2)))\n",
    "#The predicted probabilities are significantly better than guessing the train probability for every value.\n",
    "#Virtually the same (slightest bit better) than the AllDummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6571001799646914"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "AUC(testY3, predicttestprobs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5024167700184838"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC(testY3, predicttest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the \"AnyChange\" DF using other models, to see if they can give better scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
